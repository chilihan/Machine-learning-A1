{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A1:\n",
    "ENGG*6500 A1\n",
    "Train a MLP on the StumbleUpon Evergreen dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" ENGG*6500 A1\n",
    "    Train a MLP on the StumbleUpon Evergreen dataset\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import autograd.numpy as np  # Thinly-wrapped numpy\n",
    "from autograd import multigrad\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import uniform\n",
    "\n",
    "from scipy.linalg import norm\n",
    "\n",
    "# Global variables\n",
    "dtype = 'float32'\n",
    "eps = np.finfo(np.double).eps  # -- a small number\n",
    "\n",
    "\n",
    "def load_evergreen(dtype=dtype):\n",
    "    with open('evergreen.pkl') as f:\n",
    "        train_set, val_set, test_set = pickle.load(f)\n",
    "    # print train_set, val_set, test_set\n",
    "    train_X, train_y = train_set\n",
    "    # print train_set\n",
    "    val_X, val_y = val_set\n",
    "\n",
    "    return (train_X.astype(dtype), train_y.astype('int8'),\n",
    "            val_X.astype(dtype), val_y.astype('int8'))\n",
    "\n",
    "\n",
    "def logistic(z):\n",
    "    \"\"\"return logistic sigmoid of float or ndarray `z`\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def p_y_given_x(W, b, x):\n",
    "    return logistic(np.dot(x, W) + b)\n",
    "\n",
    "\n",
    "def logreg_prediction(W, b, x):\n",
    "    return p_y_given_x(W, b, x) > 0.5\n",
    "\n",
    "\n",
    "def cross_entropy(x, z):\n",
    "    # note we add a small epsilon for numerical stability\n",
    "    return -(x * np.log(z + eps) + (1 - x) * np.log(1 - z + eps))\n",
    "\n",
    "\n",
    "def logreg_cost(W, b, x, y):\n",
    "    z = p_y_given_x(W, b, x)\n",
    "    l = cross_entropy(y, z).mean(axis=0)\n",
    "    return l\n",
    "\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    return 1.0 * np.sum(y == y_pred) / y.shape[0]\n",
    "\n",
    "\n",
    "def mlp_cost(X, y, W_hid, b_hid, W_out, b_out):\n",
    "    # forward pass\n",
    "    # hidden activations\n",
    "    act_hid = p_y_given_x(W_hid, b_hid, X)\n",
    "    # output activation\n",
    "    act_out = p_y_given_x(W_out, b_out, act_hid)\n",
    "    return cross_entropy(y, act_out).mean(axis=0)\n",
    "\n",
    "\n",
    "def mlp_predict(X, W_hid, b_hid, W_out, b_out):\n",
    "    act_hid = p_y_given_x(W_hid, b_hid, X)\n",
    "    act_out = p_y_given_x(W_out, b_out, act_hid)\n",
    "    return act_out > 0.5\n",
    "\n",
    "\n",
    "def initialize_model(n_inputs, n_hidden, dtype=dtype):\n",
    "    W_hid = uniform(low=-4 * np.sqrt(6.0 / (n_inputs + n_hidden)),\n",
    "                    high=4 * np.sqrt(6.0 / (n_inputs + n_hidden)),\n",
    "                    size=(n_inputs, n_hidden)).astype(dtype)\n",
    "    b_hid = np.zeros(n_hidden, dtype=dtype)\n",
    "\n",
    "    # now allocate the logistic regression model at the top\n",
    "    W_out = uniform(low=-4 * np.sqrt(6.0 / (n_inputs + n_hidden)),\n",
    "                    high=4 * np.sqrt(6.0 / (n_inputs + n_hidden)),\n",
    "                    size=(n_hidden,)).astype(dtype)\n",
    "    b_out = np.array(0.0)\n",
    "\n",
    "    return W_hid, b_hid, W_out, b_out\n",
    "\n",
    "\n",
    "def my_grads(X, y, W_hid, b_hid, W_out, b_out):\n",
    "    \"\"\" Fill this in \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def check_gradients(cost_fn, grad_fn, X, y, W_hid, b_hid, W_out, b_out,\n",
    "                    min_diff=0.0001):\n",
    "    # generate autograd gradient function\n",
    "    # note that we specify the arguments with respect to the gradient is taken\n",
    "\n",
    "    auto_grad_fn = multigrad(cost_fn, [2, 3, 4, 5])\n",
    "    weights = [W_hid, b_hid, W_out, b_out]\n",
    "\n",
    "    grads_auto = auto_grad_fn(X, y, *weights)\n",
    "    grads_manual = grad_fn(X, y, *weights)\n",
    "    norms = [norm(auto_g - manual_g) for auto_g, manual_g in zip(grads_auto,\n",
    "                                                                 grads_manual)]\n",
    "    ret = True\n",
    "    print \"Checking gradients:\"\n",
    "    wrt = ('W_hid', 'b_hid', 'W_out', 'b_out')\n",
    "    for name, n in zip(wrt, norms):\n",
    "        if n < min_diff:\n",
    "            msg = 'OK  '\n",
    "        else:\n",
    "            msg = 'FAIL'\n",
    "            ret = False\n",
    "        print '%s: %s - norm of gradient difference with autograd: %s' % \\\n",
    "              (name, msg, n)\n",
    "\n",
    "    if ret:\n",
    "        print \"All gradients OK!\"\n",
    "    else:\n",
    "        print \"Some gradients failed.\"\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def train_model(train_X, train_y, test_X, test_y, W_hid, b_hid, W_out, b_out,\n",
    "                learning_rate, epochs, mlp_grads=None, dtype=dtype):\n",
    "    train_costs = np.zeros(epochs, dtype=dtype)\n",
    "    test_costs = np.zeros(epochs, dtype=dtype)\n",
    "\n",
    "    # Make a list of the weights\n",
    "    weights = [W_hid, b_hid, W_out, b_out]\n",
    "\n",
    "    if mlp_grads is None:\n",
    "        mlp_grads = multigrad(mlp_cost, [2, 3, 4, 5])\n",
    "    for epoch in xrange(epochs):\n",
    "        print \"Epoch\", epoch\n",
    "\n",
    "        train_cost = mlp_cost(train_X, train_y, *weights)\n",
    "        test_cost = mlp_cost(test_X, test_y, *weights)\n",
    "        train_costs[epoch] = train_cost\n",
    "        test_costs[epoch] = test_cost\n",
    "\n",
    "        grads = mlp_grads(train_X, train_y, *weights)\n",
    "\n",
    "        # returns a CudaNDarray when running on GPU\n",
    "        # creating a np.array copies the data back from the GPU\n",
    "        if not isinstance(grads[0], np.ndarray):\n",
    "            grads = [np.array(g) for g in grads]\n",
    "\n",
    "        # update the weights\n",
    "        for W, dW in zip(weights, grads):\n",
    "            W -= learning_rate * dW\n",
    "\n",
    "        print \"Training set cost:\", train_cost\n",
    "        print \"Test set cost:    \", test_cost\n",
    "\n",
    "    return train_costs, test_costs\n",
    "\n",
    "\n",
    "def run_training(n_hidden, learning_rate, epochs, data=None, model=None,\n",
    "                 grad_fn=None, show_plot=True):\n",
    "    t0 = time.time()\n",
    "    train_X, train_y, test_X, test_y = data\n",
    "\n",
    "    # initialize input layer parameters\n",
    "    n_inputs = train_X.shape[1]  # -- aka D_0\n",
    "    print \"NUM input dimensions:\", n_inputs\n",
    "\n",
    "    if model is None:\n",
    "        model = initialize_model(n_inputs, n_hidden)\n",
    "    W_hid, b_hid, W_out, b_out = model\n",
    "\n",
    "    if grad_fn is not None:\n",
    "        check = check_gradients(mlp_cost, grad_fn, train_X, train_y,\n",
    "                                W_hid, b_hid, W_out, b_out)\n",
    "        if not check:\n",
    "            print \"Failed gradient check. Aborting training\"\n",
    "            return [], []\n",
    "\n",
    "    print \"Before training\"\n",
    "    print 'train accuracy: %6.4f' % \\\n",
    "          accuracy(train_y, mlp_predict(train_X, W_hid, b_hid, W_out, b_out))\n",
    "    print 'train cross entropy: %6.4f' % \\\n",
    "          mlp_cost(train_X, train_y, W_hid, b_hid, W_out, b_out)\n",
    "    print 'test accuracy: %6.4f' % \\\n",
    "          accuracy(test_y, mlp_predict(test_X, W_hid, b_hid, W_out, b_out))\n",
    "    print 'test cross entropy: %6.4f' % mlp_cost(test_X, test_y, W_hid, b_hid,\n",
    "                                                 W_out, b_out)\n",
    "\n",
    "    train_costs, test_costs = train_model(train_X, train_y, test_X, test_y,\n",
    "                                          W_hid, b_hid, W_out, b_out,\n",
    "                                          learning_rate, epochs, grad_fn)\n",
    "\n",
    "    print \"After training, n_hidden: %s, learning_rate: %s\" % (n_hidden,\n",
    "                                                               learning_rate)\n",
    "    print 'train accuracy: %6.4f' % \\\n",
    "          accuracy(train_y, mlp_predict(train_X, W_hid, b_hid, W_out, b_out))\n",
    "    print 'train cross entropy: %6.4f' % \\\n",
    "          mlp_cost(train_X, train_y, W_hid, b_hid, W_out, b_out)\n",
    "    print 'test accuracy: %6.4f' % \\\n",
    "          accuracy(test_y, mlp_predict(test_X, W_hid, b_hid, W_out, b_out))\n",
    "    print 'test cross entropy: %6.4f' % mlp_cost(test_X, test_y, W_hid, b_hid,\n",
    "                                                 W_out, b_out)\n",
    "\n",
    "    print \"training took: %s sec\" % (time.time() - t0)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.plot(train_costs, '-b', label=\"Training data\")\n",
    "        plt.plot(test_costs, '-r', label=\"Test data\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cross entropy\")\n",
    "        plt.show()\n",
    "\n",
    "    return train_costs, test_costs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # swap the lines below once you have implemented my_grads\n",
    "    grad_fn = my_grads\n",
    "    grad_fn = None\n",
    "\n",
    "    epochs = 250\n",
    "    learning_rate = 0.01\n",
    "    n_hidden = 100  # -- aka D_1\n",
    "\n",
    "    data = load_evergreen()\n",
    "    n_inputs = data[0].shape[1]  # -- aka D_0\n",
    "\n",
    "    model = initialize_model(n_inputs, n_hidden)\n",
    "    train_costs, test_costs = run_training(n_hidden, learning_rate, epochs,\n",
    "                                           data, model, grad_fn,\n",
    "                                           show_plot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
